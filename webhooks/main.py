import asyncio
import json
import logging
import os
import sys
from collections import defaultdict
from contextlib import asynccontextmanager
from pprint import pformat
from typing import Any, Dict, List

from containers import Container
from dependency_injector.wiring import Provide, inject
from fastapi import APIRouter, Depends, FastAPI, Request, WebSocket, status
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi_websocket_pubsub import PubSubEndpoint
from services import Service

from shared_models import (WEBHOOK_TOPIC_ALL, RedisItem, TopicItem,
                           topic_mapping)

OPENAPI_NAME = os.getenv(
    "OPENAPI_NAME", "Aries Cloud API: Webhooks and Server-Sent Events")
PROJECT_VERSION = os.getenv("PROJECT_VERSION", "0.1.0")
LOG_LEVEL = os.getenv("LOG_LEVEL", "error")
log = logging.getLogger(__name__)


app = FastAPI(
    title=OPENAPI_NAME,
    description="""Welcome to the OpenAPI interface for the Aries Cloud API webhooks and Server-Sent Events (SSE) handler.
This API enables the management and processing of webhook events generated by ACA-Py instances. It supports filtering and forwarding events to subscribers based on topic and wallet ID, as well as handling Server-Sent Events (SSE) for real-time communication with clients.""",
    version=PROJECT_VERSION,
)

router = APIRouter()
endpoint = PubSubEndpoint()
endpoint.register_route(router, "/pubsub")
app.include_router(router)

# SSE related code
clients = defaultdict(list)


@asynccontextmanager
async def sse_event_stream(topic: str, service: Service):
    queue = asyncio.Queue()

    # Re-deliver undelivered messages
    undelivered_messages = await service.get_undelivered_messages(topic)
    for undelivered_message in undelivered_messages:
        await queue.put(undelivered_message)

    clients[topic].append(queue)
    try:
        yield queue
    finally:
        clients[topic].remove(queue)


async def send_sse_event(event: str, topic: str, service: Service):
    log.debug(f"Sending event to topic '{topic}': {event}")
    log.debug((f"Number of clients listening to topic '{topic}': "))
    if clients[topic]:
        for queue in clients[topic]:
            await queue.put(event)
    else:
        await service.store_undelivered_message(topic, event)


async def on_startup():
    pass  # Perform any startup tasks, if needed


async def on_shutdown():
    global clients
    # Close all open connections on shutdown
    for topic_queues in clients.values():
        for queue in topic_queues:
            await queue.put(None)
    clients = defaultdict(list)

app.add_event_handler("startup", on_startup)
app.add_event_handler("shutdown", on_shutdown)


@app.get("/sse/{walletId}", response_class=StreamingResponse, summary="Subscribe to wallet ID server-side events")
@inject
async def sse_subscribe_wallet(walletId: str, service: Service = Depends(Provide[Container.service])):
    topic = walletId

    async def event_stream():
        async with sse_event_stream(topic, service) as queue:
            while True:
                event = await queue.get()
                yield f"data: {event}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")


@app.get("/sse/{topic}/{walletId}", response_class=StreamingResponse, summary="Subscribe to topic and wallet ID server-side events")
@inject
async def sse_subscribe(topic: str, walletId: str, service: Service = Depends(Provide[Container.service])):
    topic = f"{topic}-{walletId}"

    async def event_stream():
        async with sse_event_stream(topic, service) as queue:
            while True:
                event = await queue.get()
                yield f"data: {event}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")


@app.get("/sse/{topic}", response_class=StreamingResponse, summary="Subscribe to topic server-side events")
@inject
async def sse_subscribe(topic: str, service: Service = Depends(Provide[Container.service])):
    async def event_stream():
        async with sse_event_stream(topic, service) as queue:
            while True:
                event = await queue.get()
                yield f"data: {event}\n\n"
    return StreamingResponse(event_stream(), media_type="text/event-stream")

# End of SSE related code


@app.api_route("/{topic}/{wallet_id}", summary="Subscribe or get all webhook events for a topic and wallet ID")
@inject
async def wallet_hooks(
    topic: str, wallet_id: str, service: Service = Depends(Provide[Container.service])
) -> List[TopicItem[Any]]:
    data = await service.get_all_for_topic_by_wallet_id(
        topic=topic, wallet_id=wallet_id
    )
    return data


@app.api_route("/{wallet_id}", summary="Subscribe or get all webhook events for a wallet ID")
@inject
async def wallet_root(
    wallet_id: str, service: Service = Depends(Provide[Container.service])
):
    data = await service.get_all_by_wallet(wallet_id)
    return data


# 'origin' helps to distinguish where a hook is from
# eg the admin, tenant or OP agent respectively
@app.post("/{origin}/topic/{acapy_topic}", status_code=status.HTTP_204_NO_CONTENT, summary="Receive webhook events from ACA-Py")
@inject
async def topic_root(
    acapy_topic: str,
    origin: str,
    body: Dict[str, Any],
    request: Request,
    service: Service = Depends(Provide[Container.service]),
):

    try:
        wallet_id = request.headers["x-wallet-id"]
    except KeyError:
        wallet_id = "admin"

    # We need to map from the acapy webhook topic to a unified cloud api topic. If the topic doesn't exist in the topic
    # mapping it means we don't support the webhook event yet and we will ignore it for now. This allows us to add more
    # webhook events as needed, without needing to break any models
    topic = topic_mapping.get(acapy_topic)
    if not topic:
        log.debug(
            f"Not publishing webhook event for acapy_topic {acapy_topic} as it doesn't exist in the topic_mapping"
        )
        return

    redis_item: RedisItem = RedisItem(
        payload=body,
        origin=origin,
        topic=topic,
        acapy_topic=acapy_topic,
        wallet_id=wallet_id,
    )

    webhook_event = await service.transform_topic_entry(redis_item)
    if not webhook_event:
        log.debug(
            f"Not publishing webhook event for topic {topic} as no transformer exists for the topic"
        )
        return

    # Publish the webhook event to SSE clients
    await send_sse_event(webhook_event.json(), topic, service)
    await send_sse_event(webhook_event.json(), redis_item["wallet_id"], service)
    await send_sse_event(webhook_event.json(), f"{topic}-{redis_item['wallet_id']}", service)
    await send_sse_event(webhook_event.json(), WEBHOOK_TOPIC_ALL, service)

    # publish the webhook to subscribers for the following topics
    #  - current wallet id
    #  - topic of the event
    #  - topic and wallet id combined as topic-wallet_id (this allows for fine grained subscriptions (i.e. the endorser service))
    #  - 'all' topic, which allows to subscribe to all published events
    # FIXME: wallet_id is admin for all admin wallets from different origins. We should make a difference on this
    # Maybe the wallet id should be the role (governance, tenant-admin)?
    await endpoint.publish(
        topics=[
            topic,
            redis_item["wallet_id"],
            f"{topic}-{redis_item['wallet_id']}",
            WEBHOOK_TOPIC_ALL,
        ],
        data=webhook_event.json(),
    )
    # Add data to redis
    await service.add_topic_entry(redis_item["wallet_id"], json.dumps(redis_item))

    log.debug(f"{topic}:\n{pformat(webhook_event)}")


# Example for broadcasting from eg Redis
# @router.websocket("/pubsub")
# async def websocket_rpc_endpoint(websocket: WebSocket):
#     async with endpoint.broadcaster:
#         await endpoint.main_loop(websocket)

app.include_router(router)

container = Container()
container.config.redis_host.from_env("REDIS_HOST", "wh-redis")
container.config.redis_password.from_env("REDIS_PASSWORD", "")
container.wire(modules=[sys.modules[__name__]])
